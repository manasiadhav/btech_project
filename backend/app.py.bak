
import os
from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
import numpy as np
import joblib
from datetime import datetime, timedelta
import jwt
from sklearn.ensemble import IsolationForest

# Initialize Flask app first
app = Flask(__name__)
# Enable CORS for all routes with more permissive settings
CORS(app, resources={r"/api/*": {"origins": "*"}}, supports_credentials=True)

# Add basic error handling
@app.errorhandler(404)
def not_found(e):
    return jsonify({"error": "Route not found"}), 404

@app.errorhandler(500)
def server_error(e):
    return jsonify({"error": "Internal server error"}), 500

# Constants
APP_ROOT = os.path.dirname(__file__)
DATA_PATH = os.path.join(APP_ROOT, '..', 'RPA_Bot_Data_Synthetic_800_Rows.csv')
MODEL_PATH = os.path.join(APP_ROOT, 'model.pkl')
JWT_SECRET = os.environ.get('JWT_SECRET', 'supersecretkey')
JWT_ALGO = 'HS256'

# JWT decorator
def require_jwt(fn):
    def wrapper(*args, **kwargs):
        auth = request.headers.get('Authorization', None)
        if not auth or not auth.startswith('Bearer '):
            return jsonify({'error':'Missing or invalid token'}), 401
        token = auth.split(' ')[1]
        try:
            jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGO])
        except Exception as e:
            return jsonify({'error':'Token invalid','detail':str(e)}), 401
        return fn(*args, **kwargs)
    wrapper.__name__ = fn.__name__
    return wrapper

def load_data():
    if os.path.exists(DATA_PATH):
        df = pd.read_csv(DATA_PATH)
    else:
        # fallback: small synthetic dataframe
        df = pd.DataFrame({
            'Bot Name': ['bot1','bot2','bot3'],
            'Success Rate (%)': [90, 75, 60],
            'Average Execution Time (s)': [2.3, 5.1, 7.2],
            'Last Status': ['successfully ran','failed','pending'],
            'Run Count': [100, 200, 50],
            'Failure Count': [2, 20, 10],
            'Owner': ['user1','user2','user3'],
            'Last Run Timestamp': ['2023-01-01 00:00','2023-01-02 12:00','2023-01-03 00:00'],
            'Version': ['v1.0','v1.1','v2.0'],
            'Priority': ['Low','High','Medium']
        })
    # convert timestamp
    try:
        df['Last Run Timestamp'] = pd.to_datetime(df['Last Run Timestamp'], errors='coerce')
    except Exception:
        pass
    return df

@app.route('/api/auth', methods=['POST'])
def auth():
    # Simple username/password (demo only)
    data = request.json or {}
    user = data.get('username')
    pw = data.get('password')
    # Replace with real user check
    if user == 'admin' and pw == 'admin123':
        payload = {'user': user, 'exp': datetime.utcnow() + timedelta(hours=2)}
        token = jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALGO)
        return jsonify({'token': token})
    return jsonify({'error':'Invalid credentials'}), 401

def load_risk_model():
    model_path = os.path.join(APP_ROOT, 'bot_failure_risk_model.pkl')
    if os.path.exists(model_path):
        try:
            model_data = joblib.load(model_path)
            print('Loaded risk model successfully')
            return model_data
        except Exception as e:
            print('Risk model load error:', e)
            return None
    return None

DATA_DF = load_data()
RISK_MODEL = load_risk_model()
MODEL = None  # We're using RISK_MODEL now instead

@app.route('/')
def root():
    return jsonify({
        'status': 'API is running',
        'endpoints': [
            '/api/health',
            '/api/overview',
            '/api/errors',
            '/api/performance',
            '/api/predictions',
            '/api/alerts',
            '/api/summary'
        ]
    })

@app.route('/api/health')
def health():
    return jsonify({'status':'ok','time': datetime.utcnow().isoformat()})

@app.route('/api/overview')
def overview():
    df = DATA_DF.copy()
    
    # Apply filters from query parameters
    # Get filter parameters with defaults for date range
    start_date = request.args.get('start_date', '2022-12-31')
    end_date = request.args.get('end_date', '2023-02-19')
    bot_type = request.args.get('bot_type')
    status = request.args.get('status')
    priority = request.args.get('priority')
    owner = request.args.get('owner')

    # Ensure dates are within valid range
    start_date = max(start_date, '2022-12-31')
    end_date = min(end_date, '2023-02-19')

    if start_date:
        df = df[df['Last Run Timestamp'] >= start_date]
    if end_date:
        df = df[df['Last Run Timestamp'] <= end_date]
    if bot_type:
        df = df[df['Bot Type'].str.lower() == bot_type.lower()]
    if status:
        df = df[df['Last Status'].str.lower() == status.lower()]
    if priority:
        df = df[df['Priority'].str.lower() == priority.lower()]
    if owner:
        df = df[df['Owner'] == owner]

    # Calculate overall metrics
    total_bots = df.shape[0]
    active = df[~df['Last Status'].str.contains('failed|pending', na=False, case=False)].shape[0]
    avg_success = float(df['Success Rate (%)'].mean()) if 'Success Rate (%)' in df else None
    avg_exec = float(df['Average Execution Time (s)'].mean()) if 'Average Execution Time (s)' in df else None
    uptime = round(avg_success,2) if avg_success is not None else None

    # Generate time series data
    if not df.empty:
        df['date'] = pd.to_datetime(df['Last Run Timestamp']).dt.date
        time_series = []
        
        for date, group in df.groupby('date'):
            time_series.append({
                'timestamp': date.isoformat(),
                'active_bots': int(group[~group['Last Status'].str.contains('failed|pending', na=False, case=False)].shape[0]),
                'success_rate': float(group['Success Rate (%)'].mean()),
                'avg_execution_time': float(group['Average Execution Time (s)'].mean())
            })
        
        # Sort by date
        time_series = sorted(time_series, key=lambda x: x['timestamp'])
    else:
        time_series = []

    return jsonify({
        'total_bots': int(total_bots),
        'active_bots': int(active),
        'avg_success_rate': uptime,
        'avg_execution_time_s': avg_exec,
        'time_series': time_series
    })

@app.route('/api/errors')
def errors():
    try:
        df = DATA_DF.copy()
        # Get the user filter from query parameters
        selected_user = request.args.get('user', '')
        
        # Get unique users from the Owner column
        users = sorted(df['Owner'].unique().tolist()) if 'Owner' in df else []
        
        # Filter data if user is selected
        filtered_df = df[df['Owner'] == selected_user] if selected_user else df
        print(f"Filtered data for user {selected_user}: {len(filtered_df)} rows")  # Debug log
        
        # Calculate failure by status for filtered data
        by_status = {}
        if 'Last Status' in filtered_df.columns and 'Failure Count' in filtered_df.columns:
            # Group by status and sum failures, ensuring we're using only the filtered data
            status_failures = filtered_df.groupby('Last Status', as_index=False)['Failure Count'].sum()
            # Create a dictionary from the grouped data, ensuring correct conversion to integers
            by_status = {str(row['Last Status']): int(row['Failure Count']) 
                        for _, row in status_failures.iterrows() 
                        if row['Failure Count'] > 0}
            print(f"Status failures for user {selected_user}: {by_status}")  # Debug log
        print(f"Failure by status: {by_status}")  # Debug log
        
        # Get recent entries from filtered data
        recent = filtered_df.sort_values('Last Run Timestamp', ascending=False).head(20).to_dict(orient='records')
        
        response_data = {
            'failure_by_status': by_status,
            'recent': recent,
            'users': users,
            'selected_user': selected_user  # Include selected user in response for verification
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f"Error in errors endpoint: {str(e)}")  # Debug log
        return jsonify({'error': str(e)}), 500

@app.route('/api/performance')
def performance():
    df = DATA_DF
    # Return both per-bot performance metrics and time series data
    perf = []
    for _, row in df.iterrows():
        perf.append({
            'bot_name': row.get('Bot Name'),
            'avg_execution_time_s': row.get('Average Execution Time (s)'),
            'success_rate': row.get('Success Rate (%)')
        })
    
    # Generate time series data for success rate graph
    df['date'] = pd.to_datetime(df['Last Run Timestamp']).dt.date
    time_series = []
    for date, group in df.groupby('date'):
        time_series.append({
            'date': date.isoformat(),
            'avg_success_rate': float(group['Success Rate (%)'].mean()),
            'total_bots': len(group)
        })
    
    # Sort by date
    time_series = sorted(time_series, key=lambda x: x['date'])
    
    return jsonify({
        'performance': perf,
        'time_series': time_series
    })

@app.route('/api/predictions', methods=['POST','GET'])
def predictions():
    df = DATA_DF
    # If model exists, accept bot features in POST and return prediction
    if request.method == 'POST':
        # Require JWT
        auth = request.headers.get('Authorization', None)
        if not auth or not auth.startswith('Bearer '):
            return jsonify({'error':'Missing or invalid token'}), 401
        token = auth.split(' ')[1]
        try:
            jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGO])
        except Exception as e:
            return jsonify({'error':'Token invalid','detail':str(e)}), 401
        payload = request.json or {}
        # Document expected input: features as in CSV, e.g. Success Rate (%), Failure Count, Run Count, etc.
        if MODEL is not None:
            try:
                # If model is a pipeline, it will preprocess features
                X = pd.DataFrame([payload])
                pred = MODEL.predict(X)
                proba = None
                if hasattr(MODEL, 'predict_proba'):
                    proba = MODEL.predict_proba(X).max(axis=1).tolist()[0]
                return jsonify({'prediction': pred.tolist(), 'confidence': proba})
            except Exception as e:
                return jsonify({'error':'prediction failed','detail':str(e),'input':payload}), 500
        else:
            # naive heuristic: if success rate < 70 or failure count/run_count > 0.1 predict fail
            sr = float(payload.get('Success Rate (%)', 100))
            failures = float(payload.get('Failure Count', 0))
            runs = float(payload.get('Run Count', 1))
            score = 1.0 - (sr/100.0)
            frac = failures/runs if runs>0 else 0
            risk = score + frac
            will_fail = risk > 0.3
            return jsonify({'prediction': 'will_fail' if will_fail else 'ok', 'confidence': round(min(risk,1.0),2)})

    # GET returns next predicted failures for dataset (simple heuristic or model)
    preds = []
    if MODEL is not None:
        # Use model to predict for all rows
        try:
            X = df.drop(columns=['Bot Name','Owner','Version','Priority','Last Status','Last Run Timestamp'], errors='ignore')
            pred = MODEL.predict(X)
            for i, row in df.iterrows():
                preds.append({'bot_name': row.get('Bot Name'), 'prediction': pred[i]})
        except Exception as e:
            # fallback to heuristic
            pass
    if not preds:
        for _, row in df.iterrows():
            sr = row.get('Success Rate (%)')
            failures = row.get('Failure Count')
            runs = row.get('Run Count') if row.get('Run Count') else 1
            score = 1.0 - (float(sr)/100.0) if sr is not None else 0
            frac = float(failures)/float(runs)
            risk = score + frac
            preds.append({'bot_name': row.get('Bot Name'), 'risk_score': round(float(risk),3), 'predicted_failure': risk>0.3})
        preds = sorted(preds, key=lambda x: x.get('risk_score',0), reverse=True)
    return jsonify({'predictions': preds[:50]})

@app.route('/api/alerts')
def alerts():
    # Simulate recent alerts
    df = DATA_DF
    alerts = []
    for i, row in df.head(10).iterrows():
        alerts.append({
            'bot_name': row.get('Bot Name'),
            'alert_type': 'High Failure Rate' if row.get('Failure Count',0)>20 else 'Info',
            'severity': 'critical' if row.get('Failure Count',0)>40 else 'warning' if row.get('Failure Count',0)>20 else 'info',
            'timestamp': str(row.get('Last Run Timestamp'))
        })
    return jsonify({'alerts': alerts})

@app.route('/api/summary')
def summary():
    try:
        df = DATA_DF.copy()
        if df is None or df.empty:
            return jsonify({'error': 'No data available'}), 500
        
        # Get user filter from query parameters
        user = request.args.get('user')
        print(f"Processing summary request for user: {user}")  # Debug log
        
        # Filter by user if specified
        if user and user != 'all':
            df = df[df['Owner'] == user]
            if df.empty:
                print(f"No data found for user {user}")  # Debug log
                return jsonify({'error': f'No data found for user {user}'}), 404
    
    # Get list of bots for the filtered data
    bots_list = []
    for _, row in df.iterrows():
        # Calculate risk score using Random Forest model if available
        try:
            if RISK_MODEL and all(col in row for col in RISK_MODEL['numeric_cols'] + RISK_MODEL['categorical_cols']):
                # Prepare features for the model
                features = pd.DataFrame([[
                    row.get('Run Count', 0),
                    row.get('Failure Count', 0),
                    row.get('Success Rate (%)', 0),
                    row.get('Average Execution Time (s)', 0),
                    row.get('Bot Type', ''),
                    row.get('Owner', '')
                ]], columns=RISK_MODEL['numeric_cols'] + RISK_MODEL['categorical_cols'])
                
                # Get risk probability from model
                bot_risk = float(RISK_MODEL['model'].predict_proba(features)[0, 1])
                print(f"Risk score for {row.get('Bot Name')}: {bot_risk} (from model)")  # Debug log
            else:
                # Fallback to simple calculation
                failures = float(row.get('Failure Count', 0))
                runs = float(row.get('Run Count', 1))  # Default to 1 to avoid division by zero
                success_rate = float(row.get('Success Rate (%)', 0))
                
                # Weighted risk calculation
                failure_ratio = failures / runs if runs > 0 else 0
                success_penalty = (100 - success_rate) / 100
                bot_risk = min(1.0, (failure_ratio * 0.7) + (success_penalty * 0.3))
                print(f"Risk score for {row.get('Bot Name')}: {bot_risk} (from calculation)")  # Debug log
        except Exception as e:
            print(f"Error calculating risk for bot {row.get('Bot Name')}: {e}")  # Debug log
            bot_risk = 0.0  # Safe default
        
        risk_level = 'HIGH RISK' if bot_risk > 0.7 else 'MEDIUM RISK' if bot_risk > 0.3 else 'LOW RISK'
        bots_list.append({
            'id': row['Bot Name'],
            'name': f"{row['Bot Name']} ({risk_level})",
            'risk_level': risk_level,
            'risk_score': round(bot_risk * 100, 1)
        })
    
    try:
        summary = {
            'total_runs': int(df['Run Count'].sum()) if 'Run Count' in df.columns else 0,
            'total_failures': int(df['Failure Count'].sum()) if 'Failure Count' in df.columns else 0,
            'global_success_rate': float(df['Success Rate (%)'].mean()) if 'Success Rate (%)' in df.columns else 0.0,
            'bots_with_critical_priority': (
                int(df[df['Priority'].str.lower().str.contains('critical', na=False)].shape[0]) 
                if 'Priority' in df.columns else 0
            ),
            'bots': sorted(bots_list, key=lambda x: x['risk_score'], reverse=True)
        }
        
        # Validate data
        if summary['total_runs'] < summary['total_failures']:
            print("Warning: Total failures exceed total runs")  # Debug log
            summary['total_failures'] = summary['total_runs']  # Correct inconsistency
            
        if not 0 <= summary['global_success_rate'] <= 100:
            print(f"Warning: Invalid success rate {summary['global_success_rate']}")  # Debug log
            summary['global_success_rate'] = max(0, min(100, summary['global_success_rate']))  # Clamp to valid range
            
        print(f"Generated summary: {summary}")  # Debug log
        return jsonify({'summary': summary})
        
    except Exception as e:
        print(f"Error generating summary: {e}")  # Debug log
        return jsonify({
            'error': 'Failed to generate summary',
            'detail': str(e)
        }), 500

@app.route('/api/analytics/dashboard')
def analytics():
    df = DATA_DF.copy()
    
    # Calculate summary metrics
    total_runs = int(df['Run Count'].sum()) if 'Run Count' in df else 0
    avg_success = float(df['Success Rate (%)'].mean()) if 'Success Rate (%)' in df else 0
    avg_exec = float(df['Average Execution Time (s)'].mean()) if 'Average Execution Time (s)' in df else 0
    total_errors = int(df['Failure Count'].sum()) if 'Failure Count' in df else 0
    
    # Get unique users
    users = df['Owner'].unique().tolist() if 'Owner' in df else []
    
    # Calculate per-user bot statistics
    userBots = {}
    for user in users:
        user_df = df[df['Owner'] == user]
        userBots[user] = [{
            'name': row['Bot Name'],
            'total_runs': int(row['Run Count']),
            'success_rate': float(row['Success Rate (%)']),
            'avg_exec_time': float(row['Average Execution Time (s)']),
            'error_count': int(row['Failure Count'])
        } for _, row in user_df.iterrows()]
    
    # Calculate status distribution
    status_dist = df['Last Status'].value_counts().to_dict() if 'Last Status' in df else {}
    
    # Calculate risk analysis
    risk_analysis = []
    for _, row in df.iterrows():
        risk_prob = float(row['Failure Count']) / float(row['Run Count']) if row['Run Count'] > 0 else 0
        if risk_prob > 0.3:  # Only include high risk bots
            risk_analysis.append({
                'Bot_Name': row['Bot Name'],
                'Owner': row['Owner'],
                'Risk_Prob': risk_prob
            })
    
    # Calculate daily trends
    daily_trends = {
        'Run Count': {},
        'Success Rate (%)': {}
    }
    
    if 'Last Run Timestamp' in df.columns:
        df['date'] = pd.to_datetime(df['Last Run Timestamp']).dt.date
        for date, group in df.groupby('date'):
            date_str = date.strftime('%Y-%m-%d')
            daily_trends['Run Count'][date_str] = int(group['Run Count'].sum())
            daily_trends['Success Rate (%)'][date_str] = float(group['Success Rate (%)'].mean())
    
    # Calculate owner insights
    owner_insights = {
        'Bot Name': {},
        'Success Rate (%)': {},
        'Average Execution Time (s)': {},
        'Error Count': {}
    }
    
    for owner in users:
        owner_data = df[df['Owner'] == owner]
        owner_insights['Bot Name'][owner] = len(owner_data)
        owner_insights['Success Rate (%)'][owner] = float(owner_data['Success Rate (%)'].mean())
        owner_insights['Average Execution Time (s)'][owner] = float(owner_data['Average Execution Time (s)'].mean())
        owner_insights['Error Count'][owner] = int(owner_data['Failure Count'].sum())
    
    return jsonify({
        'total_runs': total_runs,
        'success_rate': avg_success,
        'avg_execution_time': avg_exec,
        'total_errors': total_errors,
        'users': users,
        'userBots': userBots,
        'status_distribution': status_dist,
        'daily_trends': daily_trends,
        'risk_analysis': risk_analysis,
        'owner_insights': owner_insights
    })



@app.route('/api/analysis/<bot_id>')
def get_bot_analysis(bot_id):
    try:
        df = DATA_DF.copy()
        if df.empty:
            return jsonify({'error': 'No data available'}), 500
        
        # Find the bot and get its data
        matching_bots = df[df['Bot Name'] == bot_id]
        if matching_bots.empty:
            return jsonify({'error': f'Bot not found: {bot_id}'}), 404
        bot_data = matching_bots.iloc[0]
        
        # Calculate key metrics
        run_count = int(bot_data.get('Run Count', 0))
        failure_count = int(bot_data.get('Failure Count', 0))
        avg_exec_time = float(bot_data.get('Average Execution Time (s)', 0))
        
        # Calculate rates
        failure_rate = (failure_count / run_count * 100) if run_count > 0 else 0
        success_rate = 100 - failure_rate  # Success rate is complement of failure rate
        
        print(f"Bot {bot_id} metrics - Runs: {run_count}, Failures: {failure_count}, " 
              f"Failure Rate: {failure_rate}%, Success Rate: {success_rate}%")  # Debug log
              
        # Risk calculation
        if RISK_MODEL:
            try:
                features = pd.DataFrame([[
                    run_count,
                    failure_count,
                    success_rate,
                    avg_exec_time,
                    bot_data.get('Bot Type', ''),
                    bot_data.get('Owner', '')
                ]], columns=RISK_MODEL['numeric_cols'] + RISK_MODEL['categorical_cols'])
                risk_probability = float(RISK_MODEL['model'].predict_proba(features)[0, 1])
            except Exception as e:
                print(f"Model prediction failed: {e}, using fallback calculation")
                risk_probability = min(1.0, (failure_rate * 0.7 + (avg_exec_time / 100) * 0.3) / 100)
        else:
            risk_probability = min(1.0, (failure_rate * 0.7 + (avg_exec_time / 100) * 0.3) / 100)

        # Anomaly detection
        is_anomaly = False
        anomaly_score = 0.0
        try:
            features = df[['Run Count', 'Failure Count', 'Success Rate (%)', 'Average Execution Time (s)']].copy()
            features = (features - features.mean()) / features.std()
            bot_metrics = features.iloc[matching_bots.index[0]]
            anomaly_score = abs(max(bot_metrics))  # Simple anomaly score based on max deviation
            is_anomaly = anomaly_score > 2.0  # More than 2 standard deviations
        except Exception as e:
            print(f"Anomaly detection failed: {e}")

        # Prepare response
        analysis = {
            'risk_probability': float(risk_probability),
            'failure_rate': round(float(failure_rate), 1),
            'success_rate': round(float(success_rate), 1),
            'recent_failures': failure_count,
            'recent_runs': run_count,
            'is_anomalous': bool(is_anomaly),
            'anomaly_score': float(anomaly_score),
            'recommendations': [
                'Monitor error logs and failure patterns',
                'Schedule routine maintenance',
                'Review performance metrics periodically'
            ] if risk_probability < 0.3 else [
                'Review recent changes and error logs',
                'Consider reducing bot workload temporarily',
                'Schedule immediate maintenance'
            ] if risk_probability < 0.7 else [
                'URGENT: Critical risk level detected',
                'Suspend bot operations until investigation complete',
                'Schedule emergency maintenance and code review'
            ]
        }
        
        # Add anomaly-specific recommendations if needed
        if is_anomaly:
            analysis['recommendations'].extend([
                'Investigate anomalous behavior',
                'Compare current metrics with historical baselines',
                'Review bot configuration and resource allocation'
            ])

        return jsonify(analysis)
        
    except Exception as e:
        print(f"Error analyzing bot {bot_id}: {e}")
        return jsonify({'error': 'Error analyzing bot', 'detail': str(e)}), 500
        
        try:
            # Calculate risk probability using model or fallback
            if RISK_MODEL:
                try:
                    # Prepare features for the model
                    features = pd.DataFrame([[
                        run_count,
                        failure_count,
                        success_rate,
                        avg_exec_time,
                        bot_data.get('Bot Type', ''),
                        bot_data.get('Owner', '')
                    ]], columns=RISK_MODEL['numeric_cols'] + RISK_MODEL['categorical_cols'])
                    
                    risk_probability = float(RISK_MODEL['model'].predict_proba(features)[0, 1])
                except Exception as e:
                    print(f"Model prediction failed: {e}, using fallback calculation")
                    # Fallback risk calculation
                    risk_probability = min(1.0, (failure_rate * 0.7 + (avg_exec_time / 100) * 0.3) / 100)
            else:
                # Fallback risk calculation if no model
                risk_probability = min(1.0, (failure_rate * 0.7 + (avg_exec_time / 100) * 0.3) / 100)
            
            print(f"Risk calculation - Rate: {failure_rate}%, Risk Score: {risk_probability}")  # Debug log        # Calculate risk probability using Random Forest model if available
        if RISK_MODEL:
            try:
                # Prepare features
                features = pd.DataFrame([[
                    run_count,
                    failure_count,
                    success_rate,
                    avg_exec_time,
                    bot_type,
                    owner
                ]], columns=RISK_MODEL['numeric_cols'] + RISK_MODEL['categorical_cols'])
                
                # Get risk probability from model
                risk_probability = float(RISK_MODEL['model'].predict_proba(features)[0, 1])
            except Exception as e:
                print(f"Error using risk model: {e}")
                # Fallback to simple calculation if model fails
                risk_probability = min(1.0, failure_count / max(run_count, 1))
        else:
            # Fallback to simple calculation if no model
            risk_probability = min(1.0, failure_count / max(run_count, 1))
        
        # Recent metrics
        recent_runs = min(run_count, 37)  # Look at last 37 runs max
        recent_failures = int((failure_count / max(run_count, 1)) * recent_runs)
        
        try:
            # Perform anomaly detection using Isolation Forest and additional checks
            features = df[['Run Count', 'Failure Count', 'Success Rate (%)', 'Average Execution Time (s)']].copy()
            
            # Calculate dynamic contamination based on risk levels
            high_risk_count = df[df['Failure Count'] / df['Run Count'] > 0.7].shape[0]
            contamination = max(0.01, min(0.5, high_risk_count / len(df)))
            
            # Scale the features to prevent dominance of large values
            features = (features - features.mean()) / features.std()
            
            # Use Isolation Forest with dynamic contamination
            iso_forest = IsolationForest(contamination=contamination, random_state=42)
            anomaly_labels = iso_forest.fit_predict(features)
            
            # Calculate normalized metrics for the current bot
            bot_metrics = {
                'run_count': (run_count - features['Run Count'].mean()) / features['Run Count'].std(),
                'failure_count': (failure_count - features['Failure Count'].mean()) / features['Failure Count'].std(),
                'success_rate': (success_rate - features['Success Rate (%)'].mean()) / features['Success Rate (%)'].std(),
                'avg_exec_time': (avg_exec_time - features['Average Execution Time (s)'].mean()) / features['Average Execution Time (s)'].std()
            }
            
            # Create feature array for anomaly detection
            bot_features = np.array([[
                bot_metrics['run_count'],
                bot_metrics['failure_count'],
                bot_metrics['success_rate'],
                bot_metrics['avg_exec_time']
            ]])
            
            # Calculate anomaly score using Isolation Forest
            base_anomaly_score = abs(iso_forest.score_samples(bot_features)[0])
            
            # Normalize anomaly score to 0-1 range
            normalized_anomaly_score = 1 - (1 / (1 + np.exp(-base_anomaly_score)))
            
            # Calculate risk score as a combination of anomaly score and other metrics
            risk_score = (normalized_anomaly_score * 0.6) + (risk_probability * 0.4)
            
            # Determine if it's an anomaly based on threshold
            is_anomaly = risk_score > 0.7
            anomaly_score = risk_score  # Use the combined risk score as anomaly score
            
            # Update risk probability to use the combined score
            risk_probability = risk_score
            
        except Exception as e:
            print(f"Error in anomaly detection: {str(e)}")
            # Provide default values if anomaly detection fails
            is_anomaly = False
            anomaly_score = 0.0
        
        # Add anomaly-specific recommendations
        # Generate dynamic recommendations based on risk score and anomaly detection
        recommendations = []
        
        if risk_probability > 0.8:
            recommendations.extend([
                'URGENT: Critical risk level detected - immediate intervention required',
                'Suspend bot operations until thorough investigation is complete',
                'Schedule emergency maintenance and code review'
            ])
        elif risk_probability > 0.6:
            recommendations.extend([
                'High risk level - prioritize investigation of error patterns',
                'Review recent changes and error logs',
                'Consider reducing bot workload temporarily'
            ])
        else:
            recommendations.extend([
                'Monitor error logs and failure patterns',
                'Schedule routine maintenance',
                'Review performance metrics periodically'
            ])
        
        if is_anomaly:
            recommendations.extend([
                'Investigate unusual behavior patterns detected by anomaly detection',
                'Compare current metrics with historical baselines',
                'Review system resources and dependencies'
            ])
        
        if is_anomaly:
            anomaly_factors = []
            bot_values = features.loc[matching_bots.index[0]]
            mean_values = features.mean()
            std_values = features.std()
            
            # Check which metrics are contributing to the anomaly
            if abs(bot_values['Run Count'] - mean_values['Run Count']) > 2 * std_values['Run Count']:
                anomaly_factors.append('unusual number of runs')
            if abs(bot_values['Failure Count'] - mean_values['Failure Count']) > 2 * std_values['Failure Count']:
                anomaly_factors.append('abnormal failure count')
            if abs(bot_values['Success Rate (%)'] - mean_values['Success Rate (%)']) > 2 * std_values['Success Rate (%)']:
                anomaly_factors.append('unusual success rate')
            if abs(bot_values['Average Execution Time (s)'] - mean_values['Average Execution Time (s)']) > 2 * std_values['Average Execution Time (s)']:
                anomaly_factors.append('irregular execution time')
            
            recommendations.extend([
                f'Investigate anomalous behavior: {", ".join(anomaly_factors)}',
                'Consider performing detailed performance analysis',
                'Review bot configuration and resource allocation'
            ])
        
        analysis = {
            'risk_probability': float(risk_probability),
            'failure_rate': round(float(failure_rate), 1),
            'success_rate': round(float(success_rate), 1),
            'recent_failures': failure_count,
            'recent_runs': run_count,
            'is_anomalous': bool(is_anomaly),
            'anomaly_score': float(anomaly_score),
            'recommendations': [
                'Investigate recent error logs and failure patterns',
                'Schedule routine maintenance',
                'Review performance metrics periodically'
            ] if risk_probability < 0.3 else [
                'Review recent changes and error logs',
                'Consider reducing bot workload temporarily',
                'Schedule immediate maintenance'
            ] if risk_probability < 0.7 else [
                'URGENT: Critical risk level detected',
                'Suspend bot operations until investigation complete',
                'Schedule emergency maintenance and code review'
            ]
        }
        
        # Add anomaly-specific recommendations
        if is_anomaly:
            analysis['recommendations'].extend([
                'Investigate anomalous behavior',
                'Compare current metrics with historical baselines',
                'Review bot configuration and resource allocation'
            ])
        
        return jsonify(analysis)
    except Exception as e:
        print(f"Error calculating analysis: {e}")
        return jsonify({'error': 'Error calculating analysis'}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
